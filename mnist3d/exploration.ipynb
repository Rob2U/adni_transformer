{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import lightning as L\n",
    "import torch\n",
    "from dataset import MNIST3DModule\n",
    "from model import LitBasicMLP\n",
    "from config import *\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# dataset.py\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# model.py\n",
    "import lightning.pytorch as L\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# config.py\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "# Training hyperparameters\n",
    "#INPUT_DIM = (28, 28, 28)\n",
    "INPUT_DIM = 28*28*28\n",
    "OUTPUT_DIM = 10\n",
    "HIDDEN_DIM = 1024\n",
    "DROPOUT = 0.2\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 1\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 15\n",
    "\n",
    "# Dataset\n",
    "DATA_DIR = \"./MNIST\"\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Compute related\n",
    "ACCELERATOR = \"cpu\"\n",
    "DEVICES = 1\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "#DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/3DMLP\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic model\n",
    "\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linearIn = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.linearOut = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linearIn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linearOut(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitBasicMLP(L.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BasicMLP(**model_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[100, 150], gamma=0.1\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log(f\"{mode}_loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{mode}_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Dataset and DataModule to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class To3D:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        #torch._C._log_api_usage_once(self)\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.expand((x.shape[0], x.shape[1], x.shape[1], x.shape[1]))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "class Flatten:\n",
    "        \n",
    "    def __init__(self) -> None:\n",
    "    #torch._C._log_api_usage_once(self)\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return torch.flatten(x)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST3DModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers, dataset_path):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset = MNIST\n",
    "\n",
    "    #execute only on 1 GPU\n",
    "    def prepare_data(self):\n",
    "        self.dataset(self.data_dir, train=True, download=True)\n",
    "        self.dataset(self.data_dir, train=False, download=True)\n",
    "        \n",
    "    #execute on every GPU\n",
    "    def setup(self, stage):\n",
    "        test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                To3D(),\n",
    "                Flatten()\n",
    "            ]\n",
    "        )\n",
    "        # For training, we add some augmentation\n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                To3D(),\n",
    "                Flatten()\n",
    "            ]\n",
    "        )\n",
    "        # Loading the training dataset. We need to split it into a training and validation part\n",
    "        # We need to do a little trick because the validation set should not use the augmentation.\n",
    "        train_dataset = self.dataset(\n",
    "            root=self.dataset_path,\n",
    "            train=True,\n",
    "            transform=train_transform,\n",
    "        )\n",
    "        val_dataset = self.dataset(\n",
    "            root=self.dataset_path,\n",
    "            train=True,\n",
    "            transform=test_transform,\n",
    "        )\n",
    "        L.seed_everything(42)\n",
    "        self.train_ds, _ = torch.utils.data.random_split(train_dataset, [55000, 5000])\n",
    "        L.seed_everything(42)\n",
    "        _, self.val_ds = torch.utils.data.random_split(val_dataset, [55000, 5000])\n",
    "\n",
    "        self.test_ds = self.dataset(\n",
    "            root=self.data_dir, train=False, transform=test_transform, download=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple tests to make sure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitBasicMLP(\n",
    "        model_kwargs={\n",
    "            \"input_dim\": INPUT_DIM,\n",
    "            \"hidden_dim\": HIDDEN_DIM,\n",
    "            \"output_dim\": OUTPUT_DIM,\n",
    "            \"dropout\": DROPOUT,\n",
    "        },\n",
    "        lr=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "dataModule = MNIST3DModule(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    dataset_path=DATA_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "dataModule.prepare_data()\n",
    "dataModule.setup(stage=\"train\")\n",
    "train_data_loader = dataModule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_data_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, label = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21952])\n"
     ]
    }
   ],
   "source": [
    "print(original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21952])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(len(dataModule.train_ds), (1,))\n",
    "index = torch.LongTensor([2])\n",
    "example = original.squeeze()\n",
    "print(example.shape)\n",
    "# example = example.index_select(0, index)\n",
    "# print(example.shape)\n",
    "# example = example.squeeze(0)\n",
    "# print(example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(example, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(model.forward(original).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\mambaforge\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3056, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._calculate_loss((original, label), mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
